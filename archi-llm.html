
<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://dotmobo.xyz/theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark), (prefers-color-scheme: no-preference)"
    href="https://dotmobo.xyz/theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark), (prefers-color-scheme: no-preference)"
          href="https://dotmobo.xyz/theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light)"
          href="https://dotmobo.xyz/theme/pygments/emacs.min.css">


  <link rel="stylesheet" type="text/css" href="https://dotmobo.xyz/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://dotmobo.xyz/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://dotmobo.xyz/theme/font-awesome/css/solid.css">

    <link href="https://dotmobo.xyz/css/custom.css" rel="stylesheet">




<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68852219-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#333333">

<meta name="author" content="Morgan" />
<meta name="description" content="Mettre en place une architecture LLM OpenAI-compatible" />
<meta name="keywords" content="Python, LLM, vLLM, Ollama, Infinity, Speaches, LiteLLM, Open WebUI">


<meta property="og:site_name" content=".mobo"/>
<meta property="og:title" content="Mettre en place une architecture LLM OpenAI-compatible"/>
<meta property="og:description" content="Mettre en place une architecture LLM OpenAI-compatible"/>
<meta property="og:locale" content="fr_FR"/>
<meta property="og:url" content="https://dotmobo.xyz/archi-llm.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2025-07-22 00:00:00+02:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://dotmobo.xyz/author/morgan.html">
<meta property="article:section" content="IA"/>
<meta property="article:tag" content="Python"/>
<meta property="article:tag" content="LLM"/>
<meta property="article:tag" content="vLLM"/>
<meta property="article:tag" content="Ollama"/>
<meta property="article:tag" content="Infinity"/>
<meta property="article:tag" content="Speaches"/>
<meta property="article:tag" content="LiteLLM"/>
<meta property="article:tag" content="Open WebUI"/>
<meta property="og:image" content="//dotmobo.xyz/images/avatar700.jpg">

  <title>.mobo &ndash; Mettre en place une architecture LLM OpenAI-compatible</title>

</head>
<body >
  <aside>
    <div>
      <a href="https://dotmobo.xyz">
        <img src="//dotmobo.xyz/images/avatar700.jpg" alt=".mobo" title=".mobo">
      </a>

      <h1>
        <a href="https://dotmobo.xyz">.mobo</a>
      </h1>

<p>Explorations d'un développeur</p>


      <ul class="social">
          <li>
            <a  class="sc-github" href="https://github.com/dotmobo" target="_blank">
              <i class="fab fa-github"></i>
            </a>
          </li>
          <li>
            <a  class="sc-twitter" href="https://twitter.com/dotmobo" target="_blank">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
          <li>
            <a  class="sc-rss" href="//dotmobo.xyz/feeds/all.atom.xml" target="_blank">
              <i class="fas fa-rss"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="https://dotmobo.xyz">Accueil</a>

      <a href="/categories.html">Catégories</a>
      <a href="/tags.html">Tags</a>
      <a href="/archives.html">Archives</a>


    </nav>

<article class="single">
  <header>
      
    <h1 id="archi-llm">Mettre en place une architecture LLM OpenAI-compatible</h1>
    <p>
      Posté le 22/07/2025 dans <a href="https://dotmobo.xyz/category/ia.html">IA</a>

    </p>
  </header>


  <div>
    <img alt="OpenAI" class="align-right" src="./images/openai.png" />
<p>On va voir comment mettre en place une architecture de LLMs compatible avec l'API d'<a class="reference external" href="https://platform.openai.com/docs/overview">OpenAI</a>, en utilisant des outils comme <strong>vLLM</strong>, <strong>Ollama</strong>, <strong>Infinity</strong>, <strong>Speaches</strong> et <strong>LiteLLM</strong>.
De ce faire, ton service sera utilisable avec n'importe quelle application ou librairie qui se base sur l'API d'OpenAI.</p>
<p>Je ne vais pas rentrer dans le détail concernant l'installation de chaque brique, car tu trouveras facilement les informations
nécessaires sur leurs sites officiels, et il existe déjà tout un tas de tutoriels sur le sujet. On va juste se contenter de
les déployer avec <strong>Docker</strong>.</p>
<div class="section" id="docker-compose">
<h2>Docker compose</h2>
<p>On initie un fichier <cite>docker-compose.yml</cite> pour orchestrer le déploiement de nos services, qu'on va alimenter au fur et à mesure.
Voici la structure de base de notre fichier, avec le réseau et les volumes que nous utiliserons. On va tout mettre dans le même docker compose pour plus de clarté
dans cet article, mais tu pourras le découper en morceaux si tu souhaites déployer plusieurs services sur plusieurs machines.</p>
<div class="highlight"><pre><span></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.8&#39;</span>

<span class="nt">networks</span><span class="p">:</span>
<span class="w">  </span><span class="nt">vllm</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">    </span><span class="nt">external</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">services</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># On va ajouter nos services ici au fur et à mesure</span>

<span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="nt">huggingface_data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">ollama_data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">infinity_data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">speaches_data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">postgresql_data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">redis_data</span><span class="p">:</span>
</pre></div>
<p>On crée un réseau externe qu'on appelle <strong>vllm</strong>, qui va nous permettre de connecter nos services entre eux.
Bien sûr en amont de tout ça il te faut <strong>Docker</strong>, <strong>Docker Compose</strong>, <strong>Cuda</strong> et les <strong>drivers NVIDIA</strong> installés sur ta machine.</p>
<p>Tu le crées via :</p>
<div class="highlight"><pre><span></span>docker<span class="w"> </span>network<span class="w"> </span>create<span class="w"> </span><span class="se">\</span>
--driver<span class="w"> </span>bridge<span class="w"> </span><span class="se">\</span>
--subnet<span class="w"> </span><span class="m">192</span>.168.0.0/24<span class="w"> </span><span class="se">\</span>
--gateway<span class="w"> </span><span class="m">192</span>.168.0.1<span class="w"> </span><span class="se">\</span>
vllm
</pre></div>
</div>
<div class="section" id="llm-principal">
<h2>LLM principal</h2>
<p>On attaque avec le déploiement de notre <strong>LLM principal</strong> :</p>
<ul class="simple">
<li><strong>Qwen 3 30B A3B</strong> est un modèle plutôt intéressant actuellement avec son mode de <strong>reasoning</strong></li>
<li><a class="reference external" href="https://docs.vllm.ai/en/latest/">vLLM</a> est le serveur d'inférence le plus populaire et le plus simple à mettre en place pour de la production</li>
</ul>
<p>On ajoute le service dans notre <cite>docker-compose.yml</cite> :</p>
<div class="highlight"><pre><span></span><span class="nt">vllm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm/vllm-openai:v0.9.2</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">runtime</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">NVIDIA_VISIBLE_DEVICES</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;all&quot;</span>
<span class="w">    </span><span class="nt">HUGGING_FACE_HUB_TOKEN</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_huggingface_token&quot;</span>
<span class="w">    </span><span class="nt">VLLM_API_KEY</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_vllm_api_key&quot;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;huggingface_data:/root/.cache/huggingface&quot;</span>
<span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;--disable-log-stats</span><span class="nv"> </span><span class="s">--disable-log-requests</span><span class="nv"> </span><span class="s">--model</span><span class="nv"> </span><span class="s">&#39;Qwen/Qwen3-30B-A3B-FP8&#39;</span><span class="nv"> </span><span class="s">--max-model-len</span><span class="nv"> </span><span class="s">131072</span><span class="nv"> </span><span class="s">--gpu-memory-utilization</span><span class="nv"> </span><span class="s">0.8</span><span class="nv"> </span><span class="s">--swap-space</span><span class="nv"> </span><span class="s">8</span><span class="nv"> </span><span class="s">--kv-cache-dtype</span><span class="nv"> </span><span class="s">&#39;auto&#39;</span><span class="nv"> </span><span class="s">--enable-auto-tool-choice</span><span class="nv"> </span><span class="s">--tool-call-parser</span><span class="nv"> </span><span class="s">&#39;hermes&#39;</span><span class="nv"> </span><span class="s">--enable-reasoning</span><span class="nv"> </span><span class="s">--reasoning-parser</span><span class="nv"> </span><span class="s">&#39;deepseek_r1&#39;</span><span class="nv"> </span><span class="s">--rope-scaling</span><span class="nv"> </span><span class="s">&#39;{\&quot;rope_type\&quot;:\&quot;yarn\&quot;,\&quot;factor\&quot;:4.0,\&quot;original_max_position_embeddings\&quot;:32768}&#39;&quot;</span>
<span class="w">  </span><span class="nt">shm_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32g</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;curl&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-sf&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;http://localhost:8000/metrics&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
<p>Au niveau des options de vLLM :</p>
<ul class="simple">
<li>on désactive les logs des requêtes pour éviter de polluer les logs</li>
<li>on utilise le max de la taille de contexte du modèle à l'aide de <strong>rope</strong></li>
<li>on active le <strong>reasoning</strong> et le <strong>tooling</strong></li>
<li>on utilise 80% du GPU pour laisser de la place pour les autres services tout en lui donnant assez de mémoire pour être tranquille</li>
</ul>
<p>On mettra systématiquement en place un <strong>healthcheck</strong> pour monitorer les services.</p>
</div>
<div class="section" id="llm-secondaire">
<h2>LLM secondaire</h2>
<p>On va désormais déployer un service <a class="reference external" href="https://ollama.ai/">Ollama</a> pour nos <strong>LLMs secondaires</strong>. Ollama n'est pas vraiment prévu pour gérer
beaucoup d'utilisateurs en parallèle, mais il va permettre de faire des tests avec d'autres modèles pour prototyper par exemple.
Ça reste un outil intéressant mais il faut juste garder en tête que lorsque tu veux vraiment des performances, bascule sur vLLM.</p>
<p>On ajoute le service dans notre <cite>docker-compose.yml</cite> :</p>
<div class="highlight"><pre><span></span><span class="nt">ollama</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:0.9.6</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">runtime</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">NVIDIA_VISIBLE_DEVICES</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;all&quot;</span>
<span class="w">    </span><span class="nt">OLLAMA_KV_CACHE_TYPE</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;q8_0&quot;</span>
<span class="w">    </span><span class="nt">OLLAMA_KEEP_ALIVE</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;24h&quot;</span>
<span class="w">    </span><span class="nt">OLLAMA_MAX_LOADED_MODELS</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2&quot;</span>
<span class="w">    </span><span class="nt">OLLAMA_FLASH_ATTENTION</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1&quot;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama_data:/root/.ollama</span>
<span class="w">  </span><span class="nt">shm_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2g</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;ollama&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;list&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;||&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;exit&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;1&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
<p>On quantifie le <strong>KV cache</strong> et on active le <strong>flash attention</strong> pour améliorer les performances.</p>
<p>Et après à travers ton container, tu pourras pull des modèles, ici par exemple on va mettre un modèle <strong>Vision</strong> :</p>
<div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>qwen2.5vl:7b-q4_K_M
</pre></div>
</div>
<div class="section" id="embedding-et-reranking">
<h2>Embedding et reranking</h2>
<p>Pour faire du <strong>RAG</strong>, tu vas avoir besoin d'un modèle d'<strong>embedding</strong> afin d'alimenter ta base de données vectorielle type <a class="reference external" href="https://qdrant.tech/">Qdrant</a> ainsi
qu'un modèle de <strong>reranking</strong> pour améliorer la pertinence des résultats.</p>
<p>Personnellement j'utilise <a class="reference external" href="https://github.com/michaelfeil/infinity">Infinity</a> qui est un petit service dédié à ces modèles. Certains partent sur <a class="reference external" href="https://huggingface.co/docs/text-generation-inference">TGI de Hugging Face</a> qui peut aussi faire l'affaire.</p>
<p>On ajoute le service dans notre <cite>docker-compose.yml</cite> :</p>
<div class="highlight"><pre><span></span><span class="nt">infinity</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">michaelf34/infinity:0.0.76</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">runtime</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">NVIDIA_VISIBLE_DEVICES</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;all&quot;</span>
<span class="w">    </span><span class="nt">DO_NOT_TRACK</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1&quot;</span>
<span class="w">    </span><span class="nt">INFINITY_ANONYMOUS_USAGE_STATS</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;0&quot;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">infinity_data:/app/.cache</span>
<span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v2 --model-id &#39;BAAI/bge-reranker-v2-m3&#39; --model-id &#39;nomic-ai/nomic-embed-text-v1.5&#39; --port 7997</span>
<span class="w">  </span><span class="nt">shm_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2g</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;curl&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-sf&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;http://localhost:7997/health&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
<p>Infinity est intéressant car il permet de déployer plusieurs modèles en même temps. Ici on utilise <strong>nomic</strong> pour l'embedding et <strong>BGE</strong> pour le reranking.</p>
</div>
<div class="section" id="text-to-speech-et-speech-to-text">
<h2>Text-to-speech et Speech-to-text</h2>
<p><a class="reference external" href="https://github.com/speaches-ai/speaches">Speaches</a> est un service qui permet de déployer des modèles <strong>TTS</strong> et <strong>STT</strong> en mode OpenAI-compatible.
Il te permettra d'utiliser les routes <strong>/audio/transcriptions</strong> et <strong>/audio/speech</strong>.</p>
<p>On ajoute le service dans notre <cite>docker-compose.yml</cite> :</p>
<div class="highlight"><pre><span></span><span class="nt">speaches</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/speaches-ai/speaches:0.8.2-cuda-12.6.3</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">runtime</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">NVIDIA_VISIBLE_DEVICES</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;all&quot;</span>
<span class="w">    </span><span class="nt">LOG_LEVEL</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;INFO&quot;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">speaches_data:/home/ubuntu/.cache/huggingface/hub</span>
<span class="w">  </span><span class="nt">shm_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2g</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;curl&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-sf&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;http://localhost:8000/health&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
<p>Depuis le container, tu pourras récupérer <strong>faster-whisper</strong> pour le STT et <strong>piper</strong> pour le TTS via :</p>
<div class="highlight"><pre><span></span>huggingface-cli<span class="w"> </span>download<span class="w"> </span>speaches-ai/piper-fr_FR-siwis-medium
huggingface-cli<span class="w"> </span>download<span class="w"> </span>Systran/faster-whisper-small
</pre></div>
</div>
<div class="section" id="orchestration-avec-litellm">
<h2>Orchestration avec LiteLLM</h2>
<p>Et voilà, tu as tous tes services prêts à être orchestrés.</p>
<p>Ok maintenant qu'on a nos différents services, on va les orchestrer avec <a class="reference external" href="https://www.litellm.ai/">LiteLLM</a>.</p>
<p>LiteLLM va se charger de faire le routage des requêtes OpenAI vers les services adéquats, en fonction de la route demandée.</p>
<p>On imagine que tu crées le fichier de configuration suivant dans <cite>/root/litellm/config.yaml</cite> :</p>
<div class="highlight"><pre><span></span><span class="nt">general_settings</span><span class="p">:</span>
<span class="w">  </span><span class="nt">master_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">your_master_key</span>
<span class="w">  </span><span class="nt">proxy_batch_write_at</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class="w">  </span><span class="nt">database_connection_pool_limit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>

<span class="nt">model_list</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Notre LLM principal</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qwen3</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">max_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65536</span>
<span class="w">      </span><span class="nt">max_input_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65536</span>
<span class="w">      </span><span class="nt">max_output_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">65536</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hosted_vllm/Qwen/Qwen3-30B-A3B-FP8</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://vllm:8000</span>
<span class="w">      </span><span class="nt">api_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">your_vllm_api_key</span>
<span class="w">  </span><span class="c1"># Notre LLM secondaire</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qwen2.5-vl</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">max_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16384</span>
<span class="w">      </span><span class="nt">max_input_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16384</span>
<span class="w">      </span><span class="nt">max_output_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16384</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/qwen2.5vl:7b-q4_K_M</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://ollama:11434</span>
<span class="w">  </span><span class="c1"># Embedding et reranking</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nomic</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">infinity/nomic-ai/nomic-embed-text-v1.5</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://infinity:7997</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embedding</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bge-reranker</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">infinity/BAAI/bge-reranker-v2-m3</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://infinity:7997</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rerank</span>
<span class="w">  </span><span class="c1"># TTS et STT</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">whisper</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai/Systran/faster-whisper-small</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://speaches:8000</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">audio_transcription</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">piper</span>
<span class="w">    </span><span class="nt">litellm_params</span><span class="p">:</span>
<span class="w">      </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai/speaches-ai/piper-fr_FR-siwis-medium</span>
<span class="w">      </span><span class="nt">api_base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://speaches:8000</span>
<span class="w">    </span><span class="nt">model_info</span><span class="p">:</span>
<span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">audio_speech</span>

<span class="nt">litellm_settings</span><span class="p">:</span>
<span class="w">  </span><span class="nt">num_retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">  </span><span class="nt">request_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3600</span>
<span class="w">  </span><span class="nt">fallbacks</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">  </span><span class="nt">allowed_fails</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">  </span><span class="nt">cooldown_time</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">  </span><span class="nt">set_verbose</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">json_logs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">cache</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">drop_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">telemetry</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
</pre></div>
<p>On a ici une configuration de base pour servir nos modèles. En amélioration, tu pourrais gérer des modèles de <strong>fallbacks</strong> et des
limites en <strong>rpm</strong> et en <strong>tpm</strong>. Si tu as plusieurs cartes graphiques, tu peux dupliquer tes modèles dessus et liteLLM se chargera
de faire la répartition de charge en fonction de la stratégie de <strong>routing</strong> que tu auras choisie.</p>
<p>On ajoute le service dans notre <cite>docker-compose.yml</cite>. Note qu'il nous faut un <strong>postgresql</strong> et un <strong>redis</strong> pour stocker les données de LiteLLM :</p>
<div class="highlight"><pre><span></span><span class="nt">redis</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">redis:8</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">redis_data:/data</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">REDIS_PASSWORD</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_redis_password&quot;</span>
<span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;redis-server&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;--appendonly&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;yes&quot;</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;redis-cli&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;ping&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>

<span class="nt">postgres</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgres:17</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nt">POSTGRES_USER</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_postgres_user&quot;</span>
<span class="w">    </span><span class="nt">POSTGRES_PASSWORD</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_postgres_password&quot;</span>
<span class="w">    </span><span class="nt">POSTGRES_DB</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;your_postgres_db&quot;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgresql_data:/var/lib/postgresql/data</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;pg_isready&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-U&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;your_postgres_user&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>

<span class="nt">litellm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/berriai/litellm:main-v1.74.0-stable</span>
<span class="w">  </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">litellm</span>
<span class="w">  </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">  </span><span class="nt">depends_on</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgres</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">redis</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LITELLM_MODE=PRODUCTION</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LITELLM_LOG=ERROR</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LITELLM_SALT_KEY=your_salt_key</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DATABASE_URL=postgresql://your_postgres_user:your_postgres_password@postgres:5432/your_postgres_db</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">REDIS_URL=redis://redis:6379/0</span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;4000:4000&quot;</span>
<span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--config /app/config.yaml --telemetry False</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/root/litellm/config.yaml:/app/config.yaml</span>
<span class="w">  </span><span class="nt">networks</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">healthcheck</span><span class="p">:</span>
<span class="w">    </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;CMD-SHELL&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;wget</span><span class="nv"> </span><span class="s">--quiet</span><span class="nv"> </span><span class="s">--tries=1</span><span class="nv"> </span><span class="s">http://localhost:4000/health/liveliness</span><span class="nv"> </span><span class="s">||</span><span class="nv"> </span><span class="s">exit</span><span class="nv"> </span><span class="s">1&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60s</span>
<span class="w">    </span><span class="nt">timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30s</span>
<span class="w">    </span><span class="nt">retries</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
<p>Et voilà, tu démarres tout ça et tu peux accéder à ton API OpenAI compatible sur <a class="reference external" href="http://localhost:4000">http://localhost:4000</a>.</p>
</div>
<div class="section" id="utilisation-de-l-api">
<h2>Utilisation de l'API</h2>
<p>Tu peux alors par exemple utiliser la librairie <strong>OpenAI</strong> en python ou utiliser <strong>curl</strong> directement pour faire des requêtes du type :</p>
<div class="highlight"><pre><span></span>curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>http://localhost:4000/chat/completions<span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer your_master_key&quot;</span><span class="w"> </span><span class="se">\</span>
-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;qwen3&quot;,</span>
<span class="s1">    &quot;messages&quot;: [</span>
<span class="s1">        {</span>
<span class="s1">            &quot;role&quot;: &quot;user&quot;,</span>
<span class="s1">            &quot;content&quot;: &quot;Hello, how are you?&quot;</span>
<span class="s1">        }</span>
<span class="s1">    ]</span>
<span class="s1">}&#39;</span>
</pre></div>
<p>Tu peux utiliser les routes :</p>
<ul class="simple">
<li><strong>/chat/completions</strong> pour les modèles de chat avec <strong>qwen3</strong> et <strong>qwen2.5-vl</strong></li>
<li><strong>/embeddings</strong> pour les modèles d'embeddings avec <strong>nomic</strong></li>
<li><strong>/rerank</strong> pour les modèles de reranking avec <strong>bge-reranker</strong></li>
<li><strong>/audio/transcriptions</strong> pour les modèles de transcription audio avec <strong>whisper</strong></li>
<li><strong>/audio/speech</strong> pour les modèles de synthèse vocale avec <strong>piper</strong></li>
</ul>
<p>Tu pourras aussi utiliser ton API sur des interfaces graphiques comme <a class="reference external" href="https://openwebui.com/">Open WebUI</a> ou <a class="reference external" href="https://librechat.ai/">LibreChat</a>.
Étant OpenAI compatible, tu pourras l'utiliser dans des workflows d'agents comme <a class="reference external" href="https://n8n.io/">N8N</a> ou encore utiliser dans des librairies comme <a class="reference external" href="https://www.langchain.com/">LangChain</a>.</p>
<p>L'archi finale que tu pourras mettre en place pourra ressembler à ça :</p>
<img alt="Architecture LLM OpenAI-compatible" class="align-center" src="./images/archi-llm.png" />
<p>Et voilà ! Tu disposes maintenant d'une architecture LLM complète, flexible et compatible avec l'écosystème OpenAI. N'hésite pas à expérimenter avec d'autres modèles et à adapter cette configuration à tes besoins. Bon déploiement !</p>
</div>

  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://dotmobo.xyz/tag/python.html">Python</a>
      <a href="https://dotmobo.xyz/tag/llm.html">LLM</a>
      <a href="https://dotmobo.xyz/tag/vllm.html">vLLM</a>
      <a href="https://dotmobo.xyz/tag/ollama.html">Ollama</a>
      <a href="https://dotmobo.xyz/tag/infinity.html">Infinity</a>
      <a href="https://dotmobo.xyz/tag/speaches.html">Speaches</a>
      <a href="https://dotmobo.xyz/tag/litellm.html">LiteLLM</a>
      <a href="https://dotmobo.xyz/tag/open-webui.html">Open WebUI</a>
    </p>
  </div>





<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'dotmobo';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Veuillez activer le JavaScript pour voir les commentaires.
</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>
  &copy; 2024  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>
Construit avec <a href="http://getpelican.com" target="_blank">Pelican</a> utilisant le thème <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a>
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://dotmobo.xyz/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="dark"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " .mobo ",
  "url" : "https://dotmobo.xyz",
  "image": "//dotmobo.xyz/images/avatar700.jpg",
  "description": ".mobo - explorations d'un développeur"
}
</script>


</body>
</html>